{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "\n",
    "https://www.geekering.com/categories/computer-vision/marcellacavalcanti/hand-tracking-and-finger-counting-in-python-with-mediapipe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tkinter import messagebox\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Initially set finger count to 0 for each cap\n",
    "    fingerCount = 0\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        # Get hand index to check label (left or right)\n",
    "        handIndex = results.multi_hand_landmarks.index(hand_landmarks)\n",
    "        handLabel = results.multi_handedness[handIndex].classification[0].label\n",
    "\n",
    "        # Set variable to keep landmarks positions (x and y)\n",
    "        handLandmarks = []\n",
    "\n",
    "        # Fill list with x and y positions of each landmark\n",
    "        for landmarks in hand_landmarks.landmark:\n",
    "          handLandmarks.append([landmarks.x, landmarks.y])\n",
    "\n",
    "        # Test conditions for each finger: Count is increased if finger is \n",
    "        #   considered raised.\n",
    "        # Thumb: TIP x position must be greater or lower than IP x position, \n",
    "        #   deppeding on hand label.\n",
    "        if handLabel == \"Left\" and handLandmarks[4][0] > handLandmarks[3][0]:\n",
    "          fingerCount = fingerCount+1\n",
    "        elif handLabel == \"Right\" and handLandmarks[4][0] < handLandmarks[3][0]:\n",
    "          fingerCount = fingerCount+1\n",
    "\n",
    "        # Other fingers: TIP y position must be lower than PIP y position, \n",
    "        #   as image origin is in the upper left corner.\n",
    "        if handLandmarks[8][1] < handLandmarks[6][1]:       #Index finger\n",
    "          fingerCount = fingerCount+1\n",
    "        if handLandmarks[12][1] < handLandmarks[10][1]:     #Middle finger\n",
    "          fingerCount = fingerCount+1\n",
    "        if handLandmarks[16][1] < handLandmarks[14][1]:     #Ring finger\n",
    "          fingerCount = fingerCount+1\n",
    "        if handLandmarks[20][1] < handLandmarks[18][1]:     #Pinky\n",
    "          fingerCount = fingerCount+1\n",
    "\n",
    "        # Draw hand landmarks \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Display finger count\n",
    "    cv2.putText(image, str(fingerCount), (50, 450), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 10)\n",
    "\n",
    "    # Display image\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "\n",
    "    if fingerCount == 10:\n",
    "      cap.release()\n",
    "\n",
    "\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "def skinmask(img):\n",
    "    hsvim = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    lower = np.array([0, 48, 80], dtype = \"uint8\")\n",
    "    upper = np.array([20, 255, 255], dtype = \"uint8\")\n",
    "    skinRegionHSV = cv.inRange(hsvim, lower, upper)\n",
    "    blurred = cv.blur(skinRegionHSV, (2,2))\n",
    "    ret, thresh = cv.threshold(blurred,0,255,cv.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "def getcnthull(mask_img):\n",
    "    contours, hierarchy = cv.findContours(mask_img, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contours = max(contours, key=lambda x: cv.contourArea(x))\n",
    "    hull = cv.convexHull(contours)\n",
    "    return contours, hull\n",
    "\n",
    "def getdefects(contours):\n",
    "    hull = cv.convexHull(contours, returnPoints=False)\n",
    "    defects = cv.convexityDefects(contours, hull)\n",
    "    return defects\n",
    "\n",
    "    \n",
    "cap = cv.VideoCapture(0) # '0' for webcam\n",
    "while cap.isOpened():\n",
    "    _, img = cap.read()\n",
    "    try:\n",
    "        mask_img = skinmask(img)\n",
    "        contours, hull = getcnthull(mask_img)\n",
    "        cv.drawContours(img, [contours], -1, (255,255,0), 2)\n",
    "        cv.drawContours(img, [hull], -1, (0, 255, 255), 2)\n",
    "        defects = getdefects(contours)\n",
    "        if defects is not None:\n",
    "            cnt = 0\n",
    "            for i in range(defects.shape[0]):  # calculate the angle\n",
    "                s, e, f, d = defects[i][0]\n",
    "                start = tuple(contours[s][0])\n",
    "                end = tuple(contours[e][0])\n",
    "                far = tuple(contours[f][0])\n",
    "                a = np.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)\n",
    "                b = np.sqrt((far[0] - start[0]) ** 2 + (far[1] - start[1]) ** 2)\n",
    "                c = np.sqrt((end[0] - far[0]) ** 2 + (end[1] - far[1]) ** 2)\n",
    "                angle = np.arccos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c))  #      cosine theorem\n",
    "                if angle <= np.pi / 2:  # angle less than 90 degree, treat as fingers\n",
    "                    cnt += 1\n",
    "                    cv.circle(img, far, 4, [0, 0, 255], -1)\n",
    "            if cnt > 0:\n",
    "                cnt = cnt+1\n",
    "            cv.putText(img, str(cnt), (0, 50), cv.FONT_HERSHEY_SIMPLEX,1, (255, 0, 0) , 2, cv.LINE_AA)\n",
    "        cv.imshow(\"img\", img)\n",
    "    except:\n",
    "        pass\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from libsvm import svmutil\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# indices to calculate pair-wise products (H, V, D1, D2)\n",
    "shifts = [[0,1], [1,0], [1,1], [-1,1]]\n",
    "# calculate pairwise components in each orientation\n",
    "for itr_shift in range(1, len(shifts) + 1):\n",
    "    OrigArr = structdis\n",
    "    reqshift = shifts[itr_shift-1] # shifting index\n",
    "    ShiftArr = reqshift\n",
    " \n",
    "    for i in range(structdis.shape[0]):\n",
    "        for j in range(structdis.shape[1]):\n",
    "            if(i + reqshift[0] >= 0 and i + reqshift[0] < structdis.shape[0] and j + reqshift[1] >= 0 and j  + reqshift[1] < structdis.shape[1]):\n",
    "               ShiftArr[i, j] = OrigArr[i + reqshift[0], j + reqshift[1]]\n",
    "            else:\n",
    "               ShiftArr[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "image_path=\"data/people_walking.jpeg\"\n",
    "image=cv2.resize(cv2.imread(image_path),(600,400))\n",
    "plt.figure( figsize = (10,10))\n",
    "plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "saliency = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "(success, saliencyMap) = saliency.computeSaliency(image)\n",
    "saliencyMap = (saliencyMap * 255).astype(\"uint8\")\n",
    "threshMap = cv2.threshold(saliencyMap, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title('test')\n",
    "plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB)) \n",
    "\n",
    "plt.figure()\n",
    "plt.title('Saliency Map')\n",
    "plt.imshow(saliencyMap) \n",
    "\n",
    "plt.figure()\n",
    "plt.title('Thresholded image')\n",
    "plt.imshow(threshMap,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prep_input(path):\n",
    "    image =Image.open(path)\n",
    "\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "\n",
    "    image = preprocess(image)\n",
    "    image.unsqueeze_(0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def decode_output(output):\n",
    "    # taken and modified from https://pytorch.org/hub/pytorch_vision_alexnet/\n",
    "    import urllib.request\n",
    "    url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "    urllib.request.urlretrieve(url, \"imagenet_classes.txt\")\n",
    "    # Read the categories\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "    with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "        categories = [s.strip() for s in f.readlines()]\n",
    "    # Show top categories per image\n",
    "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "    for i in range(top5_prob.size(0)):\n",
    "        print(categories[top5_catid[i]], top5_prob[i].item())\n",
    "    return top5_catid[0]\n",
    "\n",
    "def prep_output(img_tensor):\n",
    "    invTrans = torchvision.transforms.Compose([ torchvision.transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                    std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                            torchvision.transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                    std = [ 1., 1., 1. ]),\n",
    "                            ])\n",
    "    out = invTrans(img_tensor)[0]\n",
    "    out = out.detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def plot_maps(img1, img2,vmin=0.3,vmax=0.7, mix_val=2):\n",
    "    f = plt.figure(figsize=(15,45))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img1,vmin=vmin, vmax=vmax, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(img2, cmap = \"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(img1*mix_val+img2/mix_val, cmap = \"gray\" )\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def norm_flat_image(img):\n",
    "    grads_norm = prep_output(img)\n",
    "    grads_norm = grads_norm[:,:,0]+ grads_norm[:,:,1]+ grads_norm[:,:,2]\n",
    "    grads_norm = (grads_norm - np.min(grads_norm))/ (np.max(grads_norm)- np.min(grads_norm))\n",
    "    return grads_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(\"data/charles_russel_buffalo.jpg\", 0) # read as gray scale\n",
    "blurred = cv2.GaussianBlur(im, (7, 7), 1.166) # apply gaussian blur to the image\n",
    "blurred_sq = blurred * blurred\n",
    "sigma = cv2.GaussianBlur(im * im, (7, 7), 1.166)\n",
    "sigma = (sigma - blurred_sq) ** 0.5\n",
    "sigma = sigma + 1.0/255 # to make sure the denominator doesn't give DivideByZero Exception\n",
    "structdis = (im - blurred)/sigma # final MSCN(i, j) image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShiftArr = structdis \n",
    "# indices to calculate pair-wise products (H, V, D1, D2)\n",
    "shifts = [[0,1], [1,0], [1,1], [-1,1]]\n",
    "# calculate pairwise components in each orientation\n",
    "for itr_shift in range(1, len(shifts) + 1):\n",
    "    OrigArr = structdis\n",
    "    reqshift = shifts[itr_shift-1] # shifting index\n",
    "\n",
    "    for i in range(structdis.shape[0]):\n",
    "        for j in range(structdis.shape[1]):\n",
    "            if(i + reqshift[0] >= 0 and i + reqshift[0] < structdis.shape[0]  and j + reqshift[1] >= 0 and j  + reqshift[1] < structdis.shape[1]):\n",
    "                ShiftArr[i, j] = OrigArr[i + reqshift[0], j + reqshift[1]]\n",
    "            else:\n",
    "                ShiftArr[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.float32([[1, 0, reqshift[1]], [0, 1, reqshift[0]]])\n",
    "ShiftArr = cv2.warpAffine(OrigArr, M, (structdis.shape[1], structdis.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(features):\n",
    "    with open('models/normalize.pickle', 'rb') as handle:\n",
    "        scale_params = pickle.load(handle)\n",
    "    \n",
    "    min_ = np.array(scale_params['min_'])\n",
    "    max_ = np.array(scale_params['max_'])\n",
    "    \n",
    "    return -1 + (2.0 / (max_ - min_) * (features - min_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't open model file allmodel\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gen_svm_nodearray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m svmutil\u001b[39m.\u001b[39msvm_load_model(\u001b[39m\"\u001b[39m\u001b[39mallmodel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# create svm node array from features list\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m x, idx \u001b[39m=\u001b[39m gen_svm_nodearray(x[\u001b[39m1\u001b[39m:], isKernel\u001b[39m=\u001b[39m(model\u001b[39m.\u001b[39mparam\u001b[39m.\u001b[39mkernel_type \u001b[39m==\u001b[39m PRECOMPUTED))\n\u001b[1;32m      5\u001b[0m nr_classifier \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# fixed for svm type as EPSILON_SVR (regression)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m prob_estimates \u001b[39m=\u001b[39m (c_double \u001b[39m*\u001b[39m nr_classifier)()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_svm_nodearray' is not defined"
     ]
    }
   ],
   "source": [
    "brisque_features = np.concatenate((brisque_features, downscale_brisque_features))\n",
    "\n",
    "\n",
    "scaled_brisque_features = scale_features(brisque_features)\n",
    "\n",
    "# load the model from allmodel file\n",
    "model = svmutil.svm_load_model(\"allmodel\")\n",
    "# create svm node array from features list\n",
    "x, idx = svmutil.gen_svm_nodearray(\n",
    "        scaled_brisque_features,\n",
    "        isKernel=(model.param.kernel))\n",
    "nr_classifier = 1 # fixed for svm type as EPSILON_SVR (regression)\n",
    "prob_estimates = (c_double * nr_classifier)()\n",
    "\n",
    "# predict quality score of an image using libsvm module\n",
    "qualityscore = svmutil.libsvm.svm_predict_probability(model, x, dec_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemfreq(x):\n",
    "    items, inv = np.unique(x, return_inverse=True)\n",
    "    freq = np.bincount(inv)\n",
    "    return np.array([items, freq]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 63  96 120]\n"
     ]
    }
   ],
   "source": [
    "def get_dominant_color(img):\n",
    "    img = cv2.imread('data/charles_russel_buffalo.jpg')\n",
    "    arr = np.float32(img)\n",
    "    pixels = arr.reshape((-1, 3))\n",
    "\n",
    "    n_colors = 5\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, .1)\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    _, labels, centroids = cv2.kmeans(pixels, n_colors, None, criteria, 10, flags)\n",
    "\n",
    "    palette = np.uint8(centroids)\n",
    "    quantized = palette[labels.flatten()]\n",
    "    quantized = quantized.reshape(img.shape)\n",
    "\n",
    "    dominant_color = palette[np.argmax(itemfreq(labels)[:, -1])]\n",
    "    return dominant_color\n",
    "\n",
    "print(get_dominant_color(img='data/charles_russel_buffalo.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'convert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     apw \u001b[39m=\u001b[39m (\u001b[39mfloat\u001b[39m(np\u001b[39m.\u001b[39msum(edges_sigma1)) \u001b[39m/\u001b[39m (im\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mim\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m]))\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m apw\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mprint\u001b[39m(average_pixel_width(img\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata/charles_russel_buffalo.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "Cell \u001b[0;32mIn [25], line 3\u001b[0m, in \u001b[0;36maverage_pixel_width\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maverage_pixel_width\u001b[39m(img):\n\u001b[1;32m      2\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39mdata/charles_russel_buffalo.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     im_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(img\u001b[39m.\u001b[39;49mconvert(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m     edges_sigma1 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcanny(im_array, sigma\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m     apw \u001b[39m=\u001b[39m (\u001b[39mfloat\u001b[39m(np\u001b[39m.\u001b[39msum(edges_sigma1)) \u001b[39m/\u001b[39m (im\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mim\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'convert'"
     ]
    }
   ],
   "source": [
    "def average_pixel_width(img):\n",
    "    img = cv2.imread('data/charles_russel_buffalo.jpg')\n",
    "    im_array = np.asarray(img.convert(mode='L'))\n",
    "    edges_sigma1 = cv2.canny(im_array, sigma=3)\n",
    "    apw = (float(np.sum(edges_sigma1)) / (im.size[0]*im.size[1]))\n",
    "    return apw*100\n",
    "\n",
    "print(average_pixel_width(img='data/charles_russel_buffalo.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.vq import whiten\n",
    "from scipy.cluster.vq import kmeans\n",
    "import pandas as pd\n",
    "  \n",
    "batman_image = img.imread('data/charles_russel_buffalo.jpg')\n",
    "  \n",
    "r = []\n",
    "g = []\n",
    "b = []\n",
    "for row in batman_image:\n",
    "    for temp_r, temp_g, temp_b in row:\n",
    "        r.append(temp_r)\n",
    "        g.append(temp_g)\n",
    "        b.append(temp_b)\n",
    "   \n",
    "batman_df = pd.DataFrame({'red' : r,\n",
    "                          'green' : g,\n",
    "                          'blue' : b})\n",
    "  \n",
    "batman_df['scaled_color_red'] = whiten(batman_df['red'])\n",
    "batman_df['scaled_color_blue'] = whiten(batman_df['blue'])\n",
    "batman_df['scaled_color_green'] = whiten(batman_df['green'])\n",
    "  \n",
    "cluster_centers, _ = kmeans(batman_df[['scaled_color_red',\n",
    "                                    'scaled_color_blue',\n",
    "                                    'scaled_color_green']], 3)\n",
    "  \n",
    "dominant_colors = []\n",
    "  \n",
    "red_std, green_std, blue_std = batman_df[['red',\n",
    "                                          'green',\n",
    "                                          'blue']].std()\n",
    "  \n",
    "for cluster_center in cluster_centers:\n",
    "    red_scaled, green_scaled, blue_scaled = cluster_center\n",
    "    dominant_colors.append((\n",
    "        red_scaled * red_std / 255,\n",
    "        green_scaled * green_std / 255,\n",
    "        blue_scaled * blue_std / 255\n",
    "    ))\n",
    "  \n",
    "plt.imshow([dominant_colors])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "def RGB_HEX(color):\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(color[0]), int(color[1]), int(color[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colors(image, number_of_colors, show_chart):\n",
    "    reshaped_image = cv2.resize(image, (600, 400))\n",
    "    reshaped_image = reshaped_image.reshape(reshaped_image.shape[0]*reshaped_image.shape[1], 3)\n",
    "    clf = KMeans(n_clusters = number_of_colors)\n",
    "    labels = clf.fit_predict(reshaped_image)\n",
    "    counts = Counter(labels)\n",
    "    counts = dict(sorted(counts.items()))\n",
    "    center_colors = clf.cluster_centers_\n",
    "    ordered_colors = [center_colors[i] for i in counts.keys()]\n",
    "    hex_colors = [RGB_HEX(ordered_colors[i]) for i in counts.keys()]\n",
    "    rgb_colors = [ordered_colors[i] for i in counts.keys()]\n",
    "    if (show_chart):\n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.pie(counts.values(), labels = hex_colors, colors = hex_colors)\n",
    "    return rgb_colors \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"data/colorpic.jpg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "get_colors(img, 8, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
